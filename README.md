# Quadcopter-TD3

Using the TD3 algorithm, a variant of the deep deterministic policy gradient algorithm, I was able to train a quadcopter to takeoff from the ground and reach a given target position. The TD3 agent used a priorititized experience replay buffer to give higher importance to actions that yielded a high change in reward. The Ornstein-Uhlenbeck process was used to add noise to the quadcopter's rotor speeds in order to encourage exploration. The TD3 agent was made up of an actor and two critics. The actor would learn what actions to make in order to maximize its rewards given the quadcopter's position, and the critic networks predicted the rewards, or Q value, that the agent would receive given a state-action pair. In order to improve stability in the learning process, target networks for the critic networks and actor network were defined, and they were updated softly by applying a tau discount of 10^-3. The physics simulator used and the task which defines the reward, or objective, function can be found in physics_sim.py and task.py, respectively. As to the prioritized replay buffer, OU noise and networks, they can be found in the folder named agents.
